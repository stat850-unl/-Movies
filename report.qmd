---
format: html
editor: visual
---

```{r, eval = F}
# This code chunk contains code to install all of the dependencies
# necessary to compile and run your report, using if-statements to
# reduce install time for unnecessary code.
# It should be set to eval = F by default, so you aren't installing
# software on someone's computer without their consent.

# This works for packages that are on CRAN
if (!"dplyr" %in% installed.packages()) {
  install.packages("dplyr")
}
if (!"remotes" %in% installed.packages()) {
  install.packages("remotes")
}

# This is how to do it for a package that's only on github
if (!"emo" %in% installed.packages()) {
  remotes::install_github("hadley/emo")

```

```{r}

```

## Introduction

Introduce your dataset and basic questions for exploration. Explain any unique approaches you will take or any interesting features of your dataset that you have to overcome. Cite the source of your dataset, and briefly discuss how it was collected.

Discuss the variables in the dataset and if there are anomalies in the variables, provide some visual assessment of the anomalies and explain how they may have arisen. You should cover most of the information in your proposal data section, but should primarily use paragraphs and not lists/tables (the exception may be a list of e.g. items which were measured).

In this project, we delve into an insightful dataset originating from the [Artists in the U.S.](https://www.arts.gov/impact/research/arts-data-profile-series/adp-31/data-tables) initiative, with supplementary access available through a repository link [here](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-09-27/artists.csv). The dataset offers a comprehensive examination of the presence and distribution of Hispanic architects across various states in the United States, shedding light on the diverse landscape of workforce demographics within the realm of artistic professions.

The data, meticulously curated and maintained, serves as a valuable resource for our exploration. Its collection from reputable sources aligns with the rigor essential for statistical analysis, providing a foundation for our research inquiries. We aim to unravel key insights into the disparities and nuances within the artistic workforce across different geographical regions.

The dataset encompasses variables that encapsulate crucial information about the workforce, such as the state of data collection, the racial or ethnic categories of the workforce, the type of professions or occupations, the total number of workers, and the specific count and proportion of artists within these occupations. These variables, while fundamental to our analysis, may present anomalies that require careful consideration.

In our examination of the dataset, we have identified potential anomalies that warrant a closer look. Through visual assessments, we aim to elucidate these anomalies, providing a nuanced understanding of their origins. This exploration ensures the integrity of our subsequent analyses, fostering a comprehensive and accurate interpretation of the dataset.

As we embark on this investigative journey, our project's unique approach lies in the collaborative utilization of both Python and R programming languages for data cleaning, preprocessing, and visualization. This tandem application enables a comprehensive assessment of the dataset, capitalizing on the strengths of each language to ensure a robust analysis.

In the forthcoming sections, we will delve into the intricacies of the dataset, addressing anomalies, exploring trends, and presenting visualizations that encapsulate the diverse dimensions of the artistic workforce in the United States. Through our analytical lens, we seek to uncover patterns, correlations, and distinctive features that contribute to a richer understanding of the artistic landscape across different states and territories.

## Methods

**Data Cleaning and Preparation**

The initial dataset underwent a systematic process of cleaning and restructuring to ensure its suitability for rigorous analysis. Notably, the cleaning procedures were predominantly executed using the versatile programming language R, with key reliance on the dplyr package \[\@dplyr-package\].

To commence the cleaning process, the raw data was scrutinized for any missing values or inconsistencies in the variable entries. Instances of missing data were addressed through appropriate imputation techniques, leveraging functionalities from the dplyr package to ensure data integrity throughout the dataset.

Furthermore, anomalies within categorical variables were systematically identified and rectified. This involved scrutinizing the dataset for outliers or erroneous entries that could potentially compromise the accuracy of subsequent analyses. The dplyr package's filter and mutate functions were instrumental in isolating and addressing these anomalies effectively.

A pivotal aspect of the data cleaning process involved the transformation of variables to harmonize units or scales where necessary. This meticulous adjustment, performed using dplyr's mutate function, ensured a standardized and coherent dataset, facilitating meaningful inter-variable comparisons.

Describe any data cleaning and rearranging you needed to do to get your dataset into a workable form. Make sure to cite any packages which were important in your data cleaning process in this section. For instance, if you used dplyr, then it would be appropriate to say something like

> we used the group-apply-combine paradigm with the `dplyr` functions `group_by` and `summarize` [@dplyr-package] to generate a dataset for each day of the observation period from the 15-minute interval observations in the raw data set.

## Topic of Exploration

Here, you want to introduce the first topic you want to explore with your (newly cleaned up) data. Code to process data should be contained in chunks above this point, and those chunks should *not* be included in the report.

You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).

If you generate a figure, it should have a caption. Here's a demonstration of how to do that:

```{r iris-plot}
#| label: fig-iris
#| fig-width: 8
#| fig-height: 4 # Change the figure dimensions using fig-width/height
#| out-width: 80% # This changes the size of the figure as rendered in the text. 
#| fig-cap: "This figure shows the relationship between sepal width and petal width in irises. I've used geom_jitter to combat overplotting, as the data are measured in relatively consistent increments. The figure is drawn with `ggplot2` [@ggplot2-package]."
#| echo: false


data(iris)
library(ggplot2)
ggplot(iris, aes(x = Sepal.Width, y = Petal.Width, color = Species)) + 
  geom_jitter() + 
  xlab("Sepal Width (cm)") + ylab("Petal Width (cm)") + 
  ggtitle("Sepal and Petal Dimensions")
```

Then, you can reference @fig-iris in the text and the appropriate cross-reference will be generated.

You can find additional information about formatting figures generated from code in the [quarto documentation](https://quarto.org/docs/authoring/figures.html#computations).

## Additional Exploration topic

Add another topic here... as many as you desire, really. Make sure to include a transition between the two sections that connects the two with some sort of logical train of thought.

## Conclusion

Here, you want to summarize the main points of what you've learned from this investigation, in paragraph form.

## Tips

(delete this section from your report!)

Almost anything you might want to know about how to format output in quarto can be found [here](https://quarto.org/docs/authoring/markdown-basics.html). Feel free to email/come to office hours to figure out how to do XYZ - part of the goal of making you write this report is that I want you to know how to write e.g. a journal paper in Quarto as well, so now's the time to experiment.

If you want to know what the wordcount of your report is, you can run the following command in your terminal:

```         
pandoc --lua-filter wordcount.lua report.qmd
```

Notice that I have not pushed `_output/report.html` or the `_output/report_files/` folder to github - this is intentional. I have actually set `_output` to not show up in git, to encourage you all to NOT push the rendered files to github and to instead work from the markdown files directly.

You may find it cleaner to create a figure subdirectory and store any figures that aren't created by R/Python in that folder. I encourage you to organize this repository in a sensible way.
